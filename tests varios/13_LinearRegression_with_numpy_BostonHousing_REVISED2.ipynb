{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 200%; font-weight: bold; color: maroon;\">Linear Regression with python + numpy. Boston housing example</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shamelessly adapted from (with great thanks, of course):\n",
    "\n",
    "https://www.cs.toronto.edu/~lczhang/321/tut/tut02_ta.html\n",
    "\n",
    "We will:\n",
    "\n",
    "    set up the linear regression problem using numpy\n",
    "    show that vectorized code is faster \n",
    "    solve the linear regression problem using the closed form solution\n",
    "    solve the linear regression problem using gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "housing_data = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset is a dictionary and now let's see its keys.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check description of the dataset\n",
    "print(housing_data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To keep the example simple, we will only work with two features: INDUS and RM. The explanations of these features are in the description above.**\n",
    "\n",
    "**Notation for our matrices**\n",
    "\n",
    "X = predictors, features, generally a matrix\n",
    "\n",
    "y = target, generally a vector\n",
    "\n",
    "w = weights, a vector of coefficients (w1 ... wp)\n",
    "\n",
    "b = or beta0, intercept or constant in the linear model. Also bias -requires explanation.\n",
    "\n",
    "yhat = prediction of the target when we have some w and b estimates\n",
    "\n",
    "e = yhat - y = error or residual of the model (at some moment)\n",
    "\n",
    "(caveat this notation is different from the used in the original notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the boston data\n",
    "data = housing_data['data']\n",
    "# we will only work with two of the features: INDUS and RM\n",
    "X = data[:, [2,5]]\n",
    "y = housing_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give us an intuition of how these two features INDUS and RM affect housing prices, lets visualize the feature interactions. As expected, the more \"industrial\" a neighbourhood is, the lower the housing prices. The more rooms houses in a neighbourhood have, the higher the median housing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual plots for the two features:\n",
    "plt.title('Industrialness vs Med House Price')\n",
    "plt.scatter(X[:, 0], y)\n",
    "plt.xlabel('Industrialness')\n",
    "plt.ylabel('Med House Price')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Avg Num Rooms vs Med House Price')\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.xlabel('Avg Num Rooms')\n",
    "plt.ylabel('Med House Price')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Cost Function\n",
    "\n",
    "In lecture, we defined the cost function for a linear regression problem using the square loss:\n",
    "\n",
    "$$\\mathcal{E}(y, yhat) = \\frac{1}{2N} \\sum_{i=1}^N (y^{(i)} - yhat^{(i)})^2$$\n",
    "\n",
    "In our case, since we have two features $x_1$ and $x_2$, our linear regression model will look like this:\n",
    "\n",
    "$$\\mathcal{E}(y, yhat) = \\frac{1}{2N} \\sum_{i=1}^N (y^{(i)} - (w_1 x_1^{(i)} + w_2 x_2^{(i)} + b) )^2$$\n",
    "\n",
    "Note that b denotes the intercept. If you want to use the \"beta0\" notation this is the same:\n",
    "\n",
    "$$\\mathcal{E}(y, yhat) = \\frac{1}{2N} \\sum_{i=1}^N (y^{(i)} - (w_1 x_1^{(i)} + w_2 x_2^{(i)} + beta0) )^2$$\n",
    "\n",
    "\n",
    "We can use the above formula to compute the cost function across an entire dataset (X, y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w1, w2, b, X, y):\n",
    "    '''\n",
    "    Evaluate the cost function in a non-vectorized manner for \n",
    "    inputs `X` and targets `y`, at weights `w1`, `w2` and `b`.\n",
    "    '''\n",
    "\n",
    "    costs = 0\n",
    "    for i in range(len(y)):\n",
    "        yhat = w1 * X[i, 0] + w2 * X[i, 1] + b\n",
    "        y_i = y[i]\n",
    "        costs += 0.5 * (y_i - yhat) ** 2\n",
    "    return costs / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the cost for this hypothesis\n",
    "\n",
    "**THIS IS AN EXAMPLE OF A UNIT TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(3, 5, 20, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...is higher than this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cost(3, 5, 0., X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the cost function:\n",
    "\n",
    "Vectorization is a way to use linear algebra to represent computations like the one above.\n",
    "In Python, vectorized code written in numpy tend to be faster than code that uses a `for` loop.\n",
    "\n",
    "\n",
    "If we write the linear regression cost function using matrix computations, it would look like this:\n",
    "\n",
    "$$\\mathcal{E}(y, yhat) = \\frac{1}{2N} \\| y - (\\bf{X} \\bf{w} + b \\bf{1}) \\| ^2$$ \n",
    "\n",
    "**NOTE we are computing the result of multiplying b (a constant) by a vector of ones (1)**\n",
    "\n",
    "Following the above formula, our vectorized code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_vectorized(w1, w2, b, X, yhat):\n",
    "    '''\n",
    "    Evaluate the cost function in a vectorized manner for \n",
    "    inputs `X` and targets `t`, at weights `w1`, `w2` and `b`.\n",
    "    '''\n",
    "\n",
    "    N = len(y)\n",
    "    w = np.array([w1, w2])\n",
    "    yhat = np.dot(X, w) + b * np.ones(N)\n",
    "    return np.sum((y - yhat)**2) / (2.0 * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we are using the b (intercept, bias, constant) outside the w vector. \n",
    "\n",
    "But we can also include it in the w vector (as it happens more often):\n",
    "\n",
    "$$\\mathcal{E}(y, yhat) = \\frac{1}{2N} \\| y - \\bf{X} \\bf{w}  \\| ^2$$ \n",
    "\n",
    "\n",
    "However note the consequence of having only 2 columns in X. We must add a vector of 1's so that we can (matrix) multiply X by w (dimensions -or shape- of X (n, p+1), of w (1, p+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_vectorized_2(w1, w2, b, X, yhat):\n",
    "    '''\n",
    "    Evaluate the cost function in a vectorized manner for \n",
    "    inputs `X` and targets `t`, at weights `w1`, `w2` and `b`.\n",
    "    '''\n",
    "\n",
    "    N = len(y)\n",
    "    w = np.array([b, w1, w2])\n",
    "#    yhat = np.dot(X, w)\n",
    "    X_1 = np.concatenate([np.ones([N, 1]),\n",
    "                          X], \n",
    "                       axis=1)\n",
    "    yhat = np.dot(X_1, w)\n",
    "    return np.sum((y - yhat)**2) / (2.0 * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the vectorized code provides the same answers as the non-vectorized code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vectorized(3, 5, 20, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vectorized(3, 5, 0, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vectorized_2(3, 5, 20, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vectorized_2(3, 5, 20, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vectorized(3, 5, 0, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing speed of the vectorized vs unvectorized code\n",
    "\n",
    "We'll see below that the vectorized code already\n",
    "runs ~2x faster than the non-vectorized code! \n",
    "\n",
    "Hopefully this will convince you to always vectorized your code whenever possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for non-vectorized code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "print(cost(4, 5, 20, X, y))\n",
    "t1 = time.time()\n",
    "t2 = t1 - t0\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for vectorized code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "print(cost_vectorized(4, 5, 20, X, y))\n",
    "t1 = time.time()\n",
    "t3 = t1 - t0\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t3/t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting cost in weight space\n",
    "\n",
    "We'll plot the cost for two of our weights, assuming that bias = -22.89831573.\n",
    "\n",
    "We'll see where that number comes from later.\n",
    "\n",
    "Notice the shape of the contours are ovals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1s = np.arange(-1.0, 0.0, 0.01)\n",
    "w2s = np.arange(6.0, 10.0, 0.1)\n",
    "z_cost = []\n",
    "for w2 in w2s:\n",
    "    z_cost.append([cost_vectorized(w1, w2, -22.89831573, X, y) for w1 in w1s])\n",
    "z_cost = np.array(z_cost)\n",
    "np.shape(z_cost)\n",
    "W1, W2 = np.meshgrid(w1s, w2s)\n",
    "CS = plt.contour(W1, W2, z_cost, 25)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.title('Costs for various values of w1 and w2 for b=0')\n",
    "plt.xlabel(\"w1\")\n",
    "plt.ylabel(\"w2\")\n",
    "plt.plot([-0.33471389], [7.82205511], 'o') # this will be the minima that we'll find later\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Solution\n",
    "\n",
    "Linear regression model has an exact solution, given by (all matrix formula)\n",
    "\n",
    "$$\\mathcal w = (X^{T} X)^{-1} X y $$ \n",
    "\n",
    "To better see this in the numpy code below, we will call:\n",
    "\n",
    "$$\\mathcal A = (X^{T} X) $$ \n",
    "\n",
    "$$\\mathcal c = X y $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But first remember we must add a column of 1's to the features matrix to be able to compute the beta0 or intercept**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add an extra feature (column in the input) that are just all ones\n",
    "X_1 = np.concatenate([np.ones([np.shape(X)[0], 1]),\n",
    "                      X], \n",
    "                      axis=1)\n",
    "X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_exactly(X, yhat):\n",
    "    '''\n",
    "    Solve linear regression exactly. (fully vectorized)\n",
    "    \n",
    "    Given `X` - NxD matrix of inputs\n",
    "          `t` - target outputs\n",
    "    Returns the optimal weights as a D-dimensional vector\n",
    "    '''\n",
    "\n",
    "    N, D = np.shape(X)\n",
    "    A = np.matmul(X.T, X)\n",
    "    c = np.dot(X.T, yhat)\n",
    "    return np.matmul(np.linalg.inv(A), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_exactly(X_1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we use library code that is written for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In real life we don't want to code it directly\n",
    "np.linalg.lstsq(X_1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Function and Gradient Descent\n",
    "\n",
    "Another approach to optimize the cost function is via gradient descent.\n",
    "\n",
    "The main idea is that we compute the gradient of the cost function with respect to\n",
    "each parameter $w_j$, which will tell us how to update $w_j$ (by a small amount) to\n",
    "improve the cost function (by a small amount).\n",
    "\n",
    "In order to implement gradient descent, we need to be able to compute the *gradient*\n",
    "of the cost function with respect to a weight $w_j$:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{E}}{\\partial w_j} = \\frac{1}{N}\\sum_i x_j^{(i)}(yhat^{(i)}-y^{(i)})$$\n",
    "\n",
    "which in matrix notation will be\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{E}}{\\partial w} = \\frac{1}{N} (X^{T}(yhat - y))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized gradient function\n",
    "def gradfn(weights, X_1, y):\n",
    "    '''\n",
    "    Given `weights` - a current \"Guess\" of what our weights should be\n",
    "          `X_1` - matrix of shape (N,D) of input features with an additional column of 1s\n",
    "          `y` - target y values\n",
    "    Return gradient of each weight evaluated at the current value\n",
    "    '''\n",
    "\n",
    "    N, D = np.shape(X_1)\n",
    "    yhat = np.matmul(X_1, weights)\n",
    "    error = yhat - y\n",
    "    return np.matmul(np.transpose(X_1), error) / float(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, we can solve the optimization problem by repeatedly\n",
    "applying gradient descent on $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_via_gradient_descent(X_1, \n",
    "                               y, \n",
    "                               print_every = 5000,\n",
    "                               niter = 100000, \n",
    "                               alpha = 0.005):\n",
    "    '''\n",
    "    Given ``X_1` - matrix of shape (N,D) of input features with an additional column of 1s\n",
    "          `y` - target y values\n",
    "    Solves for linear regression weights.\n",
    "    Return weights after `niter` iterations.\n",
    "    With an additional parameter alpha or learning_rate\n",
    "    '''\n",
    "\n",
    "    N, D = np.shape(X_1)\n",
    "    # initialize all the weights to zeros\n",
    "    w = np.zeros([D])\n",
    "    for k in range(niter):\n",
    "        dw = gradfn(w, X_1, y)\n",
    "        w = w - alpha*dw\n",
    "        if k % print_every == 0:\n",
    "            print('Weight after %d iteration: %s' % (k, str(w)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_via_gradient_descent( X_1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, this was the exact result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.lstsq(X_1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
